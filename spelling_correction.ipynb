{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "spell_checker.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tilacyn/ir-itmo/blob/master/spelling_correction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vof7eB1TVrD9",
        "colab_type": "code",
        "outputId": "dbb2eb2c-e600-4e12-e5b8-c73f2c40a446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmm9F_VMYDT1",
        "colab_type": "code",
        "outputId": "7c72e45c-6003-4875-900c-c55c7e188424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!pip install python-levenshtein"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-levenshtein\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/a9/d1785c85ebf9b7dfacd08938dd028209c34a0ea3b1bcdb895208bd40a67d/python-Levenshtein-0.12.0.tar.gz (48kB)\n",
            "\r\u001b[K     |██████▊                         | 10kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 1.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from python-levenshtein) (46.3.0)\n",
            "Building wheels for collected packages: python-levenshtein\n",
            "  Building wheel for python-levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-levenshtein: filename=python_Levenshtein-0.12.0-cp36-cp36m-linux_x86_64.whl size=144796 sha256=739cbe68cfab6adb729e6019101e1a59ea44d433ac56bec48b8a31688555c3df\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/c2/93/660fd5f7559049268ad2dc6d81c4e39e9e36518766eaf7e342\n",
            "Successfully built python-levenshtein\n",
            "Installing collected packages: python-levenshtein\n",
            "Successfully installed python-levenshtein-0.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsgNONmBUQ59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "import Levenshtein\n",
        "import re\n",
        "import os\n",
        "base_path = '/content/drive/My Drive/ir-itmo/spellchecker'\n",
        "os.chdir(base_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MARlwqYI07ka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import codecs\n",
        "import collections\n",
        "import Levenshtein\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import os\n",
        "\n",
        "russian_regex = re.compile('^[а-яА-Я]+$')\n",
        "\n",
        "\n",
        "def is_russian(word):\n",
        "    return russian_regex.match(word)\n",
        "\n",
        "\n",
        "REPLACE = 'replace'\n",
        "DELETE = 'delete'\n",
        "INSERT = 'insert'\n",
        "\n",
        "\n",
        "class ErrorModel:\n",
        "    def __init__(self, a, b, c):\n",
        "        self.operation_probability = collections.defaultdict(int)\n",
        "        self.action_probability = collections.defaultdict(int)\n",
        "        self.dependent_probability = collections.defaultdict(int)\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.c = c\n",
        "\n",
        "    def calculate_word_mutations(self, word, expected_word):\n",
        "        def get_mutation(i):\n",
        "            if i not in edit_indexes:\n",
        "                return (REPLACE, word[i], word[i])\n",
        "            else:\n",
        "                cur_edit = edit_ops[0]\n",
        "                if cur_edit[0] == REPLACE:\n",
        "                    return (REPLACE, word[cur_edit[1]], expected_word[cur_edit[2]])\n",
        "                elif cur_edit[0] == DELETE:\n",
        "                    return (DELETE, word[cur_edit[1]])\n",
        "                else:\n",
        "                    return (INSERT, expected_word[cur_edit[2]])\n",
        "\n",
        "        edit_ops = Levenshtein.editops(word, expected_word)\n",
        "        edit_indexes = set(map(lambda op: op[1], edit_ops))\n",
        "        return [get_mutation(i) for i in range(len(word))]\n",
        "\n",
        "    def calculate_probabilities(self, word_mutation):\n",
        "        prev_op = None\n",
        "        for mut in word_mutation:\n",
        "            self.operation_probability[mut[0]] += 1\n",
        "            self.action_probability[mut] += 1\n",
        "            self.dependent_probability[(prev_op, mut)] += 1\n",
        "            prev_op = mut\n",
        "\n",
        "    def build(self):\n",
        "        with codecs.open(\"train.csv\", 'r', 'utf8') as train:\n",
        "            train.readline()\n",
        "            i = 0\n",
        "            for line in train:\n",
        "                word, expected_word = line.split(',')\n",
        "                word, expected_word = word.strip(), expected_word.strip()\n",
        "                if (is_russian(word)):\n",
        "                    word_mutations = self.calculate_word_mutations(word, expected_word)\n",
        "                    self.calculate_probabilities(word_mutations)\n",
        "\n",
        "    def compute_correction_probability(self, correction, prev_op):\n",
        "        return self.a * self.operation_probability[correction[0]] + \\\n",
        "               self.b * self.action_probability[correction] + \\\n",
        "               self.c * self.dependent_probability[(prev_op, correction)]\n",
        "\n",
        "\n",
        "class TrieModel:\n",
        "    def __init__(self):\n",
        "        self.frequencies = collections.defaultdict(int)\n",
        "        self.trie = self.build_trie()\n",
        "\n",
        "    def add(self, root, word, freq):\n",
        "        node = root\n",
        "        for symbol in word:\n",
        "            found_in_child = False\n",
        "            for child in node.children:\n",
        "                if child.symbol == symbol:\n",
        "                    node = child\n",
        "                    found_in_child = True\n",
        "                    break\n",
        "            if not found_in_child:\n",
        "                new_node = TrieNode(symbol)\n",
        "                node.children.append(new_node)\n",
        "                node = new_node\n",
        "        node.terminal = True\n",
        "        node.set_f(freq)\n",
        "        node.set_w(word)\n",
        "\n",
        "    def build_trie(self):\n",
        "        root = TrieNode(\"*\")\n",
        "        with codecs.open('words.csv', 'r', 'utf8') as words:\n",
        "            words.readline()\n",
        "            for line in words:\n",
        "                processed_line = line.split(',')\n",
        "                word, freq = processed_line[0].strip(), int(processed_line[1].strip())\n",
        "                if is_russian(word):\n",
        "                    self.add(root, word, freq)\n",
        "                    self.frequencies[word] = freq\n",
        "        return root\n",
        "\n",
        "    def correct_ratio(self, fix, word):\n",
        "        return self.frequencies[fix] / self.frequencies[word]\n",
        "\n",
        "    def best_ratio(self, fix1, fix2):\n",
        "        return self.frequencies[fix1] / self.frequencies[fix2]\n",
        "\n",
        "\n",
        "class TrieNode:\n",
        "    def __init__(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        self.children = []\n",
        "        self.terminal = False\n",
        "        self.w = None\n",
        "        self.f = None\n",
        "\n",
        "    def set_f(self, f):\n",
        "        self.f = f\n",
        "\n",
        "    def set_w(self, w):\n",
        "        self.w = w\n",
        "\n",
        "    def go(self, path):\n",
        "        path += self.symbol\n",
        "        print(path)\n",
        "        if self.terminal:\n",
        "            print(self.f)\n",
        "            return\n",
        "        for c in self.children:\n",
        "            c.go(path)\n",
        "\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self):\n",
        "        a = 0\n",
        "        b = 0.9\n",
        "        c = 0.1\n",
        "        self.top = 3\n",
        "        self.correct_ratio_threshold = 5.0\n",
        "        self.best_ratio_threshold = 3.0\n",
        "        self.trie_model = TrieModel()\n",
        "        self.error_model = ErrorModel(a, b, c)\n",
        "        self.error_model.build()\n",
        "\n",
        "    def _stop_recursion(self, node, word, correction_predicts, corrections_num):\n",
        "        if corrections_num > 1:\n",
        "            return True\n",
        "        if not node.terminal and not word:\n",
        "            return True\n",
        "        if node.terminal and not word:\n",
        "            correction_predicts.append((node.w, node.f))\n",
        "            return True\n",
        "\n",
        "    def _do_correct(self, node, word, correction_predicts, prev_op=None, corrections_num=0):\n",
        "        if self._stop_recursion(node, word, correction_predicts, corrections_num):\n",
        "            return\n",
        "        corrections = []\n",
        "        cur_symbol = word[0]\n",
        "        for idx in range(len(node.children)):\n",
        "            child = node.children[idx]\n",
        "            if (len(word) >= 4) or (cur_symbol == child.symbol):\n",
        "                corrections.append((REPLACE, cur_symbol, child.symbol, idx))\n",
        "        corrections_probs = list(\n",
        "            map(lambda cor: self.error_model.compute_correction_probability(cor[0:3], prev_op), corrections))\n",
        "        corrections_statistic = sorted(list(zip(corrections, corrections_probs)), key=itemgetter(1), reverse=True)\n",
        "        top_corrections = corrections_statistic[0:self.top]\n",
        "        for correction, _ in top_corrections:\n",
        "            cur_char, fixed_char, idx = correction[1:4]\n",
        "            next_node = node.children[idx]\n",
        "            self._do_correct(next_node, word[1:], correction_predicts, correction[0:3],\n",
        "                             corrections_num if cur_char == fixed_char else corrections_num + 1)\n",
        "\n",
        "    def correct(self, word):\n",
        "        if is_russian(word) and len(word) < 100:\n",
        "            correction_predicts = []\n",
        "            # print('correct word ' + word)\n",
        "            self._do_correct(self.trie_model.trie, word, correction_predicts)\n",
        "            best_corrections = sorted(correction_predicts, key=itemgetter(1), reverse=True)\n",
        "            result = best_corrections[0] if self.trie_model.correct_ratio(best_corrections[0][0],\n",
        "                                                                          word) >= self.correct_ratio_threshold else word\n",
        "            if len(best_corrections) > 1:\n",
        "                result = best_corrections[0][0] if self.trie_model.best_ratio(best_corrections[0][0],\n",
        "                                                                              best_corrections[1][\n",
        "                                                                                  0]) >= self.best_ratio_threshold else word\n",
        "            return result\n",
        "        else:\n",
        "            return word\n",
        "\n",
        "\n",
        "def create_submission(no_fix_file, submission_file):\n",
        "    spellchecker = SpellChecker()\n",
        "\n",
        "    with codecs.open(no_fix_file, 'r', 'utf8') as file:\n",
        "        with codecs.open(submission_file, 'w', 'utf8') as submission:\n",
        "            header = file.readline()\n",
        "            submission.write(header)\n",
        "            for line in file:\n",
        "                processed_line = line.split(',')\n",
        "                word = processed_line[0]\n",
        "                if is_russian(word):\n",
        "                    corrected = spellchecker.correct(word)\n",
        "                    submission.write('{},{}\\n'.format(word, corrected))\n",
        "                else:\n",
        "                    submission.write(line)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wst3Egxm1Kob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(base_path)\n",
        "create_submission('no_fix.submission.csv', 'submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SfsA_uNYBqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare(subm1, subm2):\n",
        "    result = 0\n",
        "    with codecs.open(subm1, 'r', 'utf8') as file1:\n",
        "      with codecs.open(subm2, 'r', 'utf8') as file2:\n",
        "        file1.readline()\n",
        "        file2.readline()\n",
        "        total_len = len(file1)\n",
        "        for line1, line2 in zip(file1, file2):\n",
        "            if line1 != line2:\n",
        "              result += 1\n",
        "    return total_len, result\n",
        "                    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}